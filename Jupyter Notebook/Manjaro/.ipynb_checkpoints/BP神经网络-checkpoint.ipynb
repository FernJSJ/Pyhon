{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "roman-hungarian",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>vent_time</th>\n",
       "      <th>vent_pressure</th>\n",
       "      <th>purge_time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.175</td>\n",
       "      <td>4.00</td>\n",
       "      <td>4.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.150</td>\n",
       "      <td>4.50</td>\n",
       "      <td>3.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.125</td>\n",
       "      <td>4.00</td>\n",
       "      <td>4.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.125</td>\n",
       "      <td>5.00</td>\n",
       "      <td>3.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.150</td>\n",
       "      <td>4.50</td>\n",
       "      <td>3.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.175</td>\n",
       "      <td>4.00</td>\n",
       "      <td>3.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.150</td>\n",
       "      <td>3.66</td>\n",
       "      <td>3.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.150</td>\n",
       "      <td>4.50</td>\n",
       "      <td>3.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.150</td>\n",
       "      <td>4.50</td>\n",
       "      <td>3.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.150</td>\n",
       "      <td>4.50</td>\n",
       "      <td>2.66</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.192</td>\n",
       "      <td>4.50</td>\n",
       "      <td>3.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.175</td>\n",
       "      <td>5.00</td>\n",
       "      <td>4.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.125</td>\n",
       "      <td>5.00</td>\n",
       "      <td>4.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.150</td>\n",
       "      <td>4.50</td>\n",
       "      <td>4.34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.108</td>\n",
       "      <td>4.50</td>\n",
       "      <td>3.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.150</td>\n",
       "      <td>4.50</td>\n",
       "      <td>3.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.150</td>\n",
       "      <td>5.34</td>\n",
       "      <td>3.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.175</td>\n",
       "      <td>5.00</td>\n",
       "      <td>3.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.150</td>\n",
       "      <td>4.50</td>\n",
       "      <td>3.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.125</td>\n",
       "      <td>4.00</td>\n",
       "      <td>3.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    vent_time  vent_pressure  purge_time\n",
       "0       0.175           4.00        4.00\n",
       "1       0.150           4.50        3.50\n",
       "2       0.125           4.00        4.00\n",
       "3       0.125           5.00        3.00\n",
       "4       0.150           4.50        3.50\n",
       "5       0.175           4.00        3.00\n",
       "6       0.150           3.66        3.50\n",
       "7       0.150           4.50        3.50\n",
       "8       0.150           4.50        3.50\n",
       "9       0.150           4.50        2.66\n",
       "10      0.192           4.50        3.50\n",
       "11      0.175           5.00        4.00\n",
       "12      0.125           5.00        4.00\n",
       "13      0.150           4.50        4.34\n",
       "14      0.108           4.50        3.50\n",
       "15      0.150           4.50        3.50\n",
       "16      0.150           5.34        3.50\n",
       "17      0.175           5.00        3.00\n",
       "18      0.150           4.50        3.50\n",
       "19      0.125           4.00        3.00"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np \n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import pandas as pd \n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train = pd.read_csv(\"/home/fern/Downloads/ANNData.csv\", dtype=np.float32)\n",
    "\n",
    "x = train.drop(['Response'], axis=1)\n",
    "y = train['Response']\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "reflected-camcorder",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np \n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import pandas as pd \n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "train = pd.read_csv(\"/home/fern/Downloads/ANNData.csv\", dtype=np.float32)\n",
    "\n",
    "x = train.drop(['Response'], axis=1)\n",
    "y = train['Response']\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=2021)\n",
    "\n",
    "x_train = torch.from_numpy(x_train.values)\n",
    "y_train = torch.from_numpy(y_train.values).type(torch.LongTensor)\n",
    "\n",
    "x_test = torch.from_numpy(x_test.values)\n",
    "y_test = torch.from_numpy(y_test.values).type(torch.LongTensor)\n",
    "\n",
    "x_train = x_train.to(torch.float32)\n",
    "y_train = y_train.to(torch.float32)\n",
    "x_test = x_test.to(torch.float32)\n",
    "y_test = y_test.to(torch.float32)\n",
    "\n",
    "batch_size = 2\n",
    "iteration_num = 100\n",
    "\n",
    "train = torch.utils.data.TensorDataset(x_train, y_train)\n",
    "test = torch.utils.data.TensorDataset(x_test, y_test)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train, batch_size=batch_size, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(test, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "conditional-startup",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ANNNet(\n",
      "  (hidden): Linear(in_features=3, out_features=6, bias=True)\n",
      "  (out): Linear(in_features=6, out_features=1, bias=True)\n",
      ")\n",
      "Epoch 0, Training loss 203331344.0, Validation loss 203331344.0\n",
      "Epoch 500, Training loss 5825372.0, Validation loss 5825372.0\n",
      "Epoch 1000, Training loss 5759149.0, Validation loss 5759149.0\n",
      "Epoch 1500, Training loss 5669503.5, Validation loss 5669503.5\n",
      "Epoch 2000, Training loss 5556716.0, Validation loss 5556716.0\n",
      "Epoch 2500, Training loss 5423135.0, Validation loss 5423135.0\n",
      "Epoch 3000, Training loss 5294148.5, Validation loss 5294148.5\n",
      "Epoch 3500, Training loss 5217967.0, Validation loss 5217967.0\n",
      "Epoch 4000, Training loss 5190328.0, Validation loss 5190328.0\n",
      "Epoch 4500, Training loss 5177378.5, Validation loss 5177378.5\n",
      "Epoch 5000, Training loss 5170026.0, Validation loss 5170026.0\n",
      "Epoch 5500, Training loss 5166800.5, Validation loss 5166800.5\n",
      "Epoch 6000, Training loss 5165861.0, Validation loss 5165861.0\n",
      "Epoch 6500, Training loss 5165698.0, Validation loss 5165698.0\n",
      "Epoch 7000, Training loss 5165682.5, Validation loss 5165682.5\n",
      "Epoch 7500, Training loss 5165681.5, Validation loss 5165681.5\n",
      "Epoch 8000, Training loss 5165682.0, Validation loss 5165682.0\n",
      "Epoch 8500, Training loss 5165681.5, Validation loss 5165681.5\n",
      "Epoch 9000, Training loss 5165682.0, Validation loss 5165682.0\n",
      "Epoch 9500, Training loss 5165681.5, Validation loss 5165681.5\n"
     ]
    }
   ],
   "source": [
    "# 原始\n",
    "import torch\n",
    "import numpy as np \n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import pandas as pd \n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "train = pd.read_csv(\"/home/fern/Downloads/ANNData.csv\", dtype=np.float32)\n",
    "\n",
    "x = train.drop(['Response'], axis=1)\n",
    "y = train['Response']\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=2021)\n",
    "\n",
    "x_train = torch.from_numpy(x_train.values)\n",
    "y_train = torch.from_numpy(y_train.values).type(torch.LongTensor)\n",
    "\n",
    "x_test = torch.from_numpy(x_test.values)\n",
    "y_test = torch.from_numpy(y_test.values).type(torch.LongTensor)\n",
    "\n",
    "batch_size = 2\n",
    "iteration_num = 100\n",
    "\n",
    "class ANNNet(torch.nn.Module):\n",
    "    def __init__(self, n_input, n_hidden, n_output):\n",
    "        super(ANNNet, self).__init__()\n",
    "        self.hidden = torch.nn.Linear(n_input, n_hidden)\n",
    "        self.out = torch.nn.Linear(n_hidden, n_output)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = nn.functional.relu(self.hidden(x))\n",
    "        x = self.out(x)\n",
    "        return x\n",
    "\n",
    "net = ANNNet(n_input=3, n_hidden=6, n_output=1)\n",
    "print(net)\n",
    "optimizer = torch.optim.Adam(net.parameters(),lr=0.5)\n",
    "loss_func = torch.nn.MSELoss()\n",
    "\n",
    "for i in range(10000):\n",
    "    x_train = x_train.to(torch.float32)\n",
    "    y_train = y_train.to(torch.float32)\n",
    "    x_test = x_test.to(torch.float32)\n",
    "    y_test = y_test.to(torch.float32)\n",
    "    prediction_train = net(x_train)\n",
    "    prediction_train = prediction_train.to(torch.float32)\n",
    "    loss_train = loss_func(prediction_train, y_train)\n",
    "    \n",
    "    prediction_test = net(x_test)\n",
    "    prediction_test = prediction_test.to(torch.float32)\n",
    "    loss_test = loss_func(prediction_train, y_train)\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    loss_train.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    if i % 500 == 0:\n",
    "        print('Epoch %d, Training loss %.1f, Validation loss %.1f' % (i, float(loss_train), float(loss_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "curious-parking",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 可视化\n",
    "import torch\n",
    "import numpy as np \n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import pandas as pd \n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "train = pd.read_csv(\"/home/fern/Downloads/ANNData.csv\", dtype=np.float32)\n",
    "\n",
    "x = train.drop(['Response'], axis=1)\n",
    "y = train['Response']\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=2021)\n",
    "\n",
    "x_train = torch.from_numpy(x_train.values)\n",
    "y_train = torch.from_numpy(y_train.values).type(torch.LongTensor)\n",
    "\n",
    "x_test = torch.from_numpy(x_test.values)\n",
    "y_test = torch.from_numpy(y_test.values).type(torch.LongTensor)\n",
    "\n",
    "batch_size = 2\n",
    "iteration_num = 100\n",
    "\n",
    "class ANNNet(torch.nn.Module):\n",
    "    def __init__(self, n_input, n_hidden, n_output):\n",
    "        super(ANNNet, self).__init__()\n",
    "        self.hidden = torch.nn.Linear(n_input, n_hidden)\n",
    "        self.out = torch.nn.Linear(n_hidden, n_output)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = nn.functional.relu(self.hidden(x))\n",
    "        x = self.out(x)\n",
    "        return x\n",
    "\n",
    "net = ANNNet(n_input=3, n_hidden=6, n_output=1)\n",
    "print(net)\n",
    "optimizer = torch.optim.Adam(net.parameters(),lr=0.5)\n",
    "loss_func = torch.nn.MSELoss()\n",
    "\n",
    "for i in range(10000):\n",
    "    x_train = x_train.to(torch.float32)\n",
    "    y_train = y_train.to(torch.float32)\n",
    "    x_test = x_test.to(torch.float32)\n",
    "    y_test = y_test.to(torch.float32)\n",
    "    prediction_train = net(x_train)\n",
    "    prediction_train = prediction_train.to(torch.float32)\n",
    "    loss_train = loss_func(prediction_train, y_train)\n",
    "    \n",
    "    prediction_test = net(x_test)\n",
    "    prediction_test = prediction_test.to(torch.float32)\n",
    "    loss_test = loss_func(prediction_train, y_train)\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    loss_train.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    if i % 500 == 0:\n",
    "        print('Epoch %d, Training loss %.1f, Validation loss %.1f' % (i, float(loss_train), float(loss_test)))\n",
    "        \n",
    "import visdom\n",
    "vis = visdom.Visdom()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "literary-liability",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 改进\n",
    "import torch\n",
    "import numpy as np \n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import pandas as pd \n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "train = pd.read_csv(\"/home/fern/Downloads/ANNData.csv\", dtype=np.float32)\n",
    "\n",
    "x = train.drop(['Response'], axis=1)\n",
    "y = train['Response']\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=2021)\n",
    "\n",
    "x_train = torch.from_numpy(x_train.values)\n",
    "y_train = torch.from_numpy(y_train.values).type(torch.LongTensor)\n",
    "\n",
    "x_test = torch.from_numpy(x_test.values)\n",
    "y_test = torch.from_numpy(y_test.values).type(torch.LongTensor)\n",
    "\n",
    "batch_size = 2\n",
    "iteration_num = 100\n",
    "\n",
    "class ANNNet(torch.nn.Module):\n",
    "    def __init__(self, n_input, n_hidden, n_output):\n",
    "        super(ANNNet, self).__init__()\n",
    "        self.hidden = torch.nn.Linear(n_input, n_hidden)\n",
    "        self.out = torch.nn.Linear(n_hidden, n_output)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = nn.functional.relu(self.hidden(x))\n",
    "        x = self.out(x)\n",
    "        return x\n",
    "\n",
    "net = ANNNet(n_input=3, n_hidden=6, n_output=1)\n",
    "print(net)\n",
    "optimizer = torch.optim.Adam(net.parameters(),lr=0.5)\n",
    "loss_func = torch.nn.MSELoss()\n",
    "\n",
    "num_epochs = 10000\n",
    "\n",
    "for i in range(10001):\n",
    "    x_train = x_train.to(torch.float32)\n",
    "    y_train = y_train.to(torch.float32)\n",
    "    x_test = x_test.to(torch.float32)\n",
    "    y_test = y_test.to(torch.float32)\n",
    "    prediction_train = net(x_train)\n",
    "    prediction_train = prediction_train.to(torch.float32)\n",
    "    loss_train = loss_func(prediction_train, y_train)\n",
    "    \n",
    "    prediction_test = net(x_test)\n",
    "    prediction_test = prediction_test.to(torch.float32)\n",
    "    loss_test = loss_func(prediction_train, y_train)\n",
    "       \n",
    "    optimizer.zero_grad()\n",
    "    loss_train.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    if i % 500 == 0:\n",
    "        print('Epoch %d, Training loss %.1f, Validation loss %.1f' % (i, float(loss_train), float(loss_test)))  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "historical-latest",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['font.sans-serif']=['SimHei']\n",
    "plt.rcParams['axes.unicode_minus'] = False\n",
    "plt.figure(figsize=(8, 5))\n",
    "x = np.linspace(start=-2, stop=100000000,num =1000,dtype=np.float)\n",
    "logi = np.log(1 + np.exp(-x))/math.log(2)\n",
    "boost = np.exp(-x)\n",
    "y_01 = x < 0\n",
    "y_hinge = 1.0 - x\n",
    "y_hinge[y_hinge < 0] = 0\n",
    "plt.plot(x, logi, 'r-', mec='k', label='Logistic Loss', lw=2)\n",
    "plt.plot(x, y_01, 'g-', mec='k', label='0/1 Loss', lw=2)\n",
    "plt.plot(x, y_hinge, 'b-',mec='k', label='Hinge Loss', lw=2)\n",
    "plt.plot(x, boost, 'm--',mec='k', label='Adaboost Loss',lw=2)\n",
    "plt.grid(True, ls='--')\n",
    "plt.legend(loc='upper right')\n",
    "plt.title('损失函数')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "described-clothing",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "x and y must have same first dimension, but have shapes (1000,) and (1,)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-41-909cea92d73a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinspace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100000000\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnum\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r-'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmec\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'k'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Training Loss'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlw\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'g-'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmec\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'k'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Testing Loss'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlw\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/matplotlib/pyplot.py\u001b[0m in \u001b[0;36mplot\u001b[0;34m(scalex, scaley, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2838\u001b[0m \u001b[0;34m@\u001b[0m\u001b[0m_copy_docstring_and_deprecators\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mAxes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2839\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscalex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscaley\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2840\u001b[0;31m     return gca().plot(\n\u001b[0m\u001b[1;32m   2841\u001b[0m         \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscalex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mscalex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscaley\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mscaley\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2842\u001b[0m         **({\"data\": data} if data is not None else {}), **kwargs)\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/matplotlib/axes/_axes.py\u001b[0m in \u001b[0;36mplot\u001b[0;34m(self, scalex, scaley, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1741\u001b[0m         \"\"\"\n\u001b[1;32m   1742\u001b[0m         \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcbook\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormalize_kwargs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmlines\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLine2D\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1743\u001b[0;31m         \u001b[0mlines\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_lines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1744\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlines\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1745\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_line\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/matplotlib/axes/_base.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m    271\u001b[0m                 \u001b[0mthis\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    272\u001b[0m                 \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 273\u001b[0;31m             \u001b[0;32myield\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_plot_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mthis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    274\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    275\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_next_color\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/matplotlib/axes/_base.py\u001b[0m in \u001b[0;36m_plot_args\u001b[0;34m(self, tup, kwargs)\u001b[0m\n\u001b[1;32m    397\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    398\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 399\u001b[0;31m             raise ValueError(f\"x and y must have same first dimension, but \"\n\u001b[0m\u001b[1;32m    400\u001b[0m                              f\"have shapes {x.shape} and {y.shape}\")\n\u001b[1;32m    401\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: x and y must have same first dimension, but have shapes (1000,) and (1,)"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAeoAAAEwCAYAAAB4yDJzAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/Il7ecAAAACXBIWXMAAAsTAAALEwEAmpwYAAAM4UlEQVR4nO3cz4vdd73H8df7Ektq1ZDkDigRRClYKZhFRxDbCpneBMQouJMaUFwECoIIVei/INkUFCQ7CRG3aqCUmAT8QUudLBIuapFYL2QjEwqJC01BPneRA3rnTnLOTM6cvjvn8dj4mZn3zLz5OMyTMw3fGmMEAOjpP97tBQCA+xNqAGhMqAGgMaEGgMaEGgAaE2oAaGymUFfV+6rqFw/4+P6qulBV16rqXFXV/FYEgOU1NdRV9WiSq0mOP2DsVJKbY4yjSQ5OmQUAZjQ11GOMv48xPp3k5gPG1pJcnJwvJzk2h90AYOntm9PXOZzk9uR8J8knNw9U1ekkp5Pksccee+qJJ56Y07cGgP6uXr16a4yxst3Pm1eobyU5MDkfmLz9f4wxziY5mySrq6tjfX19Tt8aAPqrqv/ZyefN6199X0pyYnJeS3JlTl8XAJbatkNdVR+vqjOb3n0+yZGqup7k7dwLNwDwkGb+0/cY4/HJ/76V5MVNH7ub5OR8VwMAPPAEABoTagBoTKgBoDGhBoDGhBoAGhNqAGhMqAGgMaEGgMaEGgAaE2oAaEyoAaAxoQaAxoQaABoTagBoTKgBoDGhBoDGhBoAGhNqAGhMqAGgMaEGgMaEGgAaE2oAaEyoAaAxoQaAxoQaABoTagBoTKgBoDGhBoDGhBoAGhNqAGhMqAGgMaEGgMaEGgAaE2oAaEyoAaAxoQaAxoQaABoTagBoTKgBoDGhBoDGhBoAGhNqAGhMqAGgMaEGgMaEGgAaE2oAaEyoAaCxqaGuqv1VdaGqrlXVuaqqLWYeq6qfVdVvq+r7u7MqACyfWV5Rn0pyc4xxNMnBJMe3mPlaktfHGE8nebKqPjXHHQFgac0S6rUkFyfny0mObTFzN8n7J6+29yd5Z/NAVZ2uqvWqWt/Y2NjpvgCwVGYJ9eEktyfnO0kObTHzkyRfSPKHJH8cY9zYPDDGODvGWB1jrK6srOx0XwBYKrOE+laSA5Pzgcnbm72U5EdjjCeSHKqqz81pPwBYarOE+lKSE5PzWpIrW8x8MMk/Jue7ST7w8KsBALOE+nySI1V1PcnbSW5U1ZlNMz9M8kJVvZbk0dyLOwDwkPZNGxhj3E1yctO7X9w085ckT89vLQAg8cATAGhNqAGgMaEGgMaEGgAaE2oAaEyoAaAxoQaAxoQaABoTagBoTKgBoDGhBoDGhBoAGhNqAGhMqAGgMaEGgMaEGgAaE2oAaEyoAaAxoQaAxoQaABoTagBoTKgBoDGhBoDGhBoAGhNqAGhMqAGgMaEGgMaEGgAaE2oAaEyoAaAxoQaAxoQaABoTagBoTKgBoDGhBoDGhBoAGhNqAGhMqAGgMaEGgMaEGgAaE2oAaEyoAaAxoQaAxoQaABoTagBoTKgBoLGpoa6q/VV1oaquVdW5qqr7zH2vqn5dVa9U1SPzXxUAls8sr6hPJbk5xjia5GCS45sHquoTSZ4cYzyb5JUkH53rlgCwpGYJ9VqSi5Pz5STHtph5LsnBqvpVkmeTvDWf9QBguc0S6sNJbk/Od5Ic2mJmJcnGGOPzufdq+pnNA1V1uqrWq2p9Y2Njp/sCwFKZJdS3khyYnA9M3t7sTpI3J+c/JzmyeWCMcXaMsTrGWF1ZWdnJrgCwdGYJ9aUkJybntSRXtpi5muQzk/PjuRdrAOAhzRLq80mOVNX1JG8nuVFVZ/59YIzxWpJbVfW7JG+OMd6Y/6oAsHz2TRsYY9xNcnLTu1/cYu6FeS0FANzjgScA0JhQA0BjQg0AjQk1ADQm1ADQmFADQGNCDQCNCTUANCbUANCYUANAY0INAI0JNQA0JtQA0JhQA0BjQg0AjQk1ADQm1ADQmFADQGNCDQCNCTUANCbUANCYUANAY0INAI0JNQA0JtQA0JhQA0BjQg0AjQk1ADQm1ADQmFADQGNCDQCNCTUANCbUANCYUANAY0INAI0JNQA0JtQA0JhQA0BjQg0AjQk1ADQm1ADQmFADQGNCDQCNCTUANCbUANCYUANAY1NDXVX7q+pCVV2rqnNVVQ+Y/U5V/XK+KwLA8prlFfWpJDfHGEeTHExyfKuhqvpYkm/MbzUAYJZQryW5ODlfTnLsPnMvJ3lpHksBAPfMEurDSW5PzneSHNo8UFXPJ7mW5Pf3+yJVdbqq1qtqfWNjYye7AsDSmSXUt5IcmJwPTN7e7GSS55L8NMlTVfWtzQNjjLNjjNUxxurKyspO9wWApTJLqC8lOTE5ryW5snlgjPH8GOOZJF9NcnWM8YP5rQgAy2uWUJ9PcqSqrid5O8mNqjqzu2sBAEmyb9rAGONu7v1p+9+9eJ/ZvyT5r4dfCwBIPPAEAFoTagBoTKgBoDGhBoDGhBoAGhNqAGhMqAGgMaEGgMaEGgAaE2oAaEyoAaAxoQaAxoQaABoTagBoTKgBoDGhBoDGhBoAGhNqAGhMqAGgMaEGgMaEGgAaE2oAaEyoAaAxoQaAxoQaABoTagBoTKgBoDGhBoDGhBoAGhNqAGhMqAGgMaEGgMaEGgAaE2oAaEyoAaAxoQaAxoQaABoTagBoTKgBoDGhBoDGhBoAGhNqAGhMqAGgMaEGgMaEGgAaE2oAaEyoAaCxB4a6qvZX1YWqulZV56qqtpipqvpxVb1eVT+vqn27ty4ALJdpr6hPJbk5xjia5GCS41vMPJ1k3xjjs0k+lOTEfFcEgOU1LdRrSS5OzpeTHNti5q9JXp6c37nfF6qq01W1XlXrGxsb214UAJbRtFAfTnJ7cr6T5NDmgTHGn8YYb1TVV5I8kuTVrb7QGOPsGGN1jLG6srLyMDsDwNKYFupbSQ5Mzgcmb/8/VfXlJN9O8qUxxj/ntx4ALLdpob6Uf/0357UkVzYPVNWHk3w3yRfHGH+b73oAsNymhfp8kiNVdT3J20luVNWZTTNfT/KRJK9W1W+q6pu7sCcALKUaYyz8m66uro719fWFf18AeLdU1dUxxup2P88DTwCgMaEGgMaEGgAaE2oAaEyoAaAxoQaAxoQaABoTagBoTKgBoDGhBoDGhBoAGhNqAGhMqAGgMaEGgMaEGgAaE2oAaEyoAaAxoQaAxoQaABoTagBoTKgBoDGhBoDGhBoAGhNqAGhMqAGgMaEGgMaEGgAaE2oAaEyoAaAxoQaAxoQaABoTagBoTKgBoDGhBoDGhBoAGhNqAGhMqAGgMaEGgMaEGgAaE2oAaEyoAaAxoQaAxoQaABoTagBoTKgBoDGhBoDGHhjqqtpfVReq6lpVnauq2skMALAz015Rn0pyc4xxNMnBJMd3OAMA7MC0UK8luTg5X05ybIczAMAO7Jvy8cNJbk/Od5J8coczqarTSU5P3rxbVf+9vVXZpv9McuvdXmIJuOfd5453nztejC37OM20UN9KcmByPpCt/4+cZSZjjLNJziZJVa2PMVa3vS0zc8eL4Z53nzvefe54MapqfSefN+1P35eSnJic15Jc2eEMALAD00J9PsmRqrqe5O0kN6rqzJSZS/NfEwCW0wP/9D3GuJvk5KZ3vzjDzDRntznP9rnjxXDPu88d7z53vBg7uucaY8x7EQBgTjyZDAAa27VQe6rZ7pvxjquqflxVr1fVz6tq2r/0Z5Pt/JxW1Xeq6peL3G8vmPWOq+p7VfXrqnqlqh5Z9J7vZTP+vnisqn5WVb+tqu+/G3vuBVX1vqr6xQM+vq327eYrak81232z3N/TSfaNMT6b5EP517/QZ3Yz/ZxW1ceSfGOBe+0lU++4qj6R5MkxxrNJXkny0cWu+J43y8/x15K8PsZ4OsmTVfWpRS64F1TVo0mu5sE921b7djPUnmq2+2a5v78meXlyfmcRS+1Bs/6cvpzkpYVstPfMcsfPJTlYVb9K8myStxa0214xyx3fTfL+ySu8/fE7Y9vGGH8fY3w6yc0HjG2rfbsZ6s1PLDu0wxnub+r9jTH+NMZ4o6q+kuSRJK8ucL+9Yuo9V9XzSa4l+f0C99pLZvldsJJkY4zx+dx7Nf3MgnbbK2a5458k+UKSPyT54xjjxoJ2Wzbbat9uhnpuTzXjvma6v6r6cpJvJ/nSGOOfC9ptL5nlnk/m3iu+nyZ5qqq+taDd9opZ7vhOkjcn5z8nObKAvfaSWe74pSQ/GmM8keRQVX1uUcstmW21bzdD7almu2/q/VXVh5N8N8kXxxh/W+Bue8nUex5jPD/GeCbJV5NcHWP8YIH77QWz/C64muQzk/PjuRdrZjfLHX8wyT8m57tJPrCAvZbRttq3m6H2VLPdN8sdfz3JR5K8WlW/qapvLnrJPWCWe+bhTL3jMcZrSW5V1e+SvDnGeONd2PO9bJaf4x8meaGqXkvyaPxOfmhV9fGHbZ8HngBAYx54AgCNCTUANCbUANCYUANAY0INAI0JNQA0JtQA0Nj/AlJlmGBKcC0HAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 576x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['font.sans-serif']=['SimHei']\n",
    "plt.rcParams['axes.unicode_minus'] = False\n",
    "plt.figure(figsize=(8, 5))\n",
    "\n",
    "Loss_train_list = []\n",
    "Loss_train_list.append(loss_train)\n",
    "Loss_test_list = []\n",
    "Loss_test_list.append(loss_test)\n",
    "\n",
    "x = range(0, 10000)\n",
    "y1 = Loss_train_list\n",
    "y2 = Loss_test_list\n",
    "x = np.linspace(start=0, stop=100000000,num =1000,dtype=np.float)\n",
    "\n",
    "plt.plot(x, y1, 'r-', mec='k', label='Training Loss', lw=2)\n",
    "plt.plot(x, y2, 'g-', mec='k', label='Testing Loss', lw=2)\n",
    "\n",
    "plt.grid(True, ls='--')\n",
    "plt.legend(loc='upper right')\n",
    "plt.title('损失函数')\n",
    "plt.show()\n",
    "# plt.savefig(\"loss.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "emerging-general",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "Loss_train_list = []\n",
    "Loss_train_list.append(loss_train/(len(x_train)))\n",
    "Loss_test_list = []\n",
    "Loss_test_list.append(loss_test/(len(x_test)))\n",
    "x1 = range(0, 10000)\n",
    "y1 = Loss_train_list\n",
    "x2 = range(0, 10000)\n",
    "y2 = Loss_test_list\n",
    "plt.subplot(2,1,1)\n",
    "plt.plot(x1, y1, 'o-')\n",
    "plt.title('Training loss of epoches')\n",
    "plt.xlabel('epoches')\n",
    "plt.ylabel('Training loss')\n",
    "    \n",
    "plt.subplot(2,1,2)\n",
    "plt.plot(x2, y2, '.-')\n",
    "plt.title('Testing loss of epoches')\n",
    "plt.xlabel('epoches')\n",
    "plt.ylabel('Testing loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "connected-knowing",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
